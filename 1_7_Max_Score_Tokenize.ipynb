{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Score Tokenizer(for space error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = {'파스': 0.3, '파스타': 0.7, '좋아요': 0.2, '좋아':0.5}\n",
    "sent = '파스타가좋아요'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('파스타', 0, 3, 0.7),\n",
      " ('좋아', 4, 6, 0.5),\n",
      " ('파스', 0, 2, 0.3),\n",
      " ('좋아요', 4, 7, 0.2),\n",
      " ('스타', 1, 3, 0),\n",
      " ('스타가', 1, 4, 0),\n",
      " ('타가', 2, 4, 0),\n",
      " ('타가좋', 2, 5, 0),\n",
      " ('가좋', 3, 5, 0),\n",
      " ('가좋아', 3, 6, 0),\n",
      " ('아요', 5, 7, 0)]\n"
     ]
    }
   ],
   "source": [
    "subtokens = []\n",
    "\n",
    "for b in range(0, len(sent)):\n",
    "    # word length is loger than 2\n",
    "    for r in range(2, 3+1):\n",
    "        \n",
    "        e = b + r\n",
    "        \n",
    "        if e > len(sent):\n",
    "            continue\n",
    "            \n",
    "        subtoken = sent[b:e]\n",
    "        \n",
    "        # (subtoken, 시작점, 끝점, 단어 점수)\n",
    "        subtokens.append((subtoken, b, e, score.get(subtoken, 0)))\n",
    "        \n",
    "# sort by score\n",
    "subtokens = sorted(subtokens, key=lambda x:x[3], reverse=True)\n",
    "pprint(subtokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subtoken = 파스타\n",
      "\n",
      "[('좋아', 4, 6, 0.5),\n",
      " ('파스', 0, 2, 0.3),\n",
      " ('좋아요', 4, 7, 0.2),\n",
      " ('스타', 1, 3, 0),\n",
      " ('스타가', 1, 4, 0),\n",
      " ('타가', 2, 4, 0),\n",
      " ('타가좋', 2, 5, 0),\n",
      " ('가좋', 3, 5, 0),\n",
      " ('가좋아', 3, 6, 0),\n",
      " ('아요', 5, 7, 0)]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "word, b, e, s = subtokens.pop(0)\n",
    "\n",
    "print('subtoken = %s\\n' % word)\n",
    "pprint(subtokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파스타와 겹치는 부분\n",
      "('파스', 0, 2, 0.3)\n",
      "('스타', 1, 3, 0)\n",
      "('스타가', 1, 4, 0)\n",
      "('타가', 2, 4, 0)\n",
      "('타가좋', 2, 5, 0)\n",
      "\n",
      "중복된 subtokens을 지운 뒤\n",
      "[('좋아', 4, 6, 0.5),\n",
      " ('좋아요', 4, 7, 0.2),\n",
      " ('가좋', 3, 5, 0),\n",
      " ('가좋아', 3, 6, 0),\n",
      " ('아요', 5, 7, 0)]\n"
     ]
    }
   ],
   "source": [
    "results.append((word, b, e, s))\n",
    "\n",
    "removals = []\n",
    "for i, (word_, b_, e_, _) in enumerate(subtokens):\n",
    "    \n",
    "    # word와 오버랩 되는 word_\n",
    "    if (b_ < e and b < e_):\n",
    "        removals.append(i)\n",
    "\n",
    "print('파스타와 겹치는 부분')\n",
    "for i in removals:\n",
    "    print(subtokens[i])\n",
    "\n",
    "print('\\n중복된 subtokens을 지운 뒤')\n",
    "for i in reversed(removals): # index 꼬임 없이 삭제위해\n",
    "    del subtokens[i]\n",
    "    \n",
    "pprint(subtokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaxScore Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sent, score, max_len=3):\n",
    "    \n",
    "    def initialize(sent, score, max_len=3):\n",
    "        subtokens = []\n",
    "        \n",
    "        for b in range(0, len(sent)):\n",
    "            for r in range(2, max_len+1):\n",
    "                \n",
    "                e = b + r\n",
    "                if e > len(sent):\n",
    "                    continue\n",
    "                    \n",
    "                subtoken = sent[b:e]\n",
    "                subtokens.append((subtoken, b, e, score.get(subtoken, 0), e - b))\n",
    "                \n",
    "        if not subtokens:\n",
    "            return subtokens\n",
    "        \n",
    "        # Sort by (score and its length)\n",
    "        subtokens = sorted(subtokens, key=lambda x:x[3], reverse=True)\n",
    "#         subtokens = sorted(subtokens, key=lambda x:(x[3], x[4]), reverse=True)\n",
    "        return subtokens\n",
    "    \n",
    "    def _tokenize(subtokens):\n",
    "        results = []\n",
    "        \n",
    "        while subtokens:\n",
    "            \n",
    "            word, b, e, s, l = subtokens.pop(0)\n",
    "            results.append((word, b, e, s, l))\n",
    "            \n",
    "            # Select overlapped subtoken\n",
    "            removals = []\n",
    "            for i, (word_, b_, e_, _1, _2) in enumerate(subtokens):\n",
    "                if (b_ < e and b < e_):\n",
    "                    removals.append(i)\n",
    "                    \n",
    "            # Remove them\n",
    "            for i in reversed(removals):\n",
    "                del subtokens[i]\n",
    "                \n",
    "        # Sort by begin point\n",
    "        results = sorted(results, key=lambda x:x[1])\n",
    "        return results\n",
    "    \n",
    "    def postprocess(sent, results):\n",
    "        # 맨 앞글자가 비었을 경우, \n",
    "        if results[0][1] != 0:\n",
    "            b = 0\n",
    "            e = results[0][1]\n",
    "            word = sent[b:e]\n",
    "            results.insert(0, (word, b, e, 0, e - b))\n",
    "            \n",
    "        # 맨 뒷글자가 비었을 경우\n",
    "        if results[-1][2] != len(sent):\n",
    "            b = results[-1][2]\n",
    "            e = len(sent)\n",
    "            word = sent[b:e]\n",
    "            results.append((word, b, e, 0, e - b))\n",
    "        \n",
    "        # 중간 글자가 비었을 경우\n",
    "        adds = []\n",
    "        for i, base in enumerate(results[:-1]):\n",
    "            if base[2] == results[i+1][1]:\n",
    "                continue            \n",
    "            b = base[2]\n",
    "            e = results[i+1][1]\n",
    "            word = sent[b:e]\n",
    "            adds.append((word, b, e, 0, e - b))\n",
    "        \n",
    "        results = sorted(results + adds, key=lambda x:x[1])\n",
    "        return results\n",
    "            \n",
    "    subtokens = initialize(sent, score, max_len)\n",
    "    if not subtokens:\n",
    "        return [(sent, 0, len(sent), 0)]\n",
    "    \n",
    "    results = _tokenize(subtokens)\n",
    "    results = postprocess(sent, results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('파스타', 0, 3, 0.7, 3),\n",
       " ('가', 3, 4, 0, 1),\n",
       " ('좋아', 4, 6, 0.5, 2),\n",
       " ('요', 6, 7, 0, 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(sent, score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
