{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pip install bs4\n",
    "# !pip install lxml\n",
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_basic_base = 'http://movie.naver.com/movie/bi/mi/basic.nhn?code=%s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_id = 150198 # 너의 이름은"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = url_basic_base % movie_id\n",
    "req = requests.get(url)\n",
    "html = req.text\n",
    "\n",
    "## parsing\n",
    "data = BeautifulSoup(html, \"html.parser\") # data = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scraping test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<strong class=\"h_movie2\" title=\"君の名は。\t\t\t\t\t\t\t\t\t, your name.\t\t\t\t\t\t\t\t\t\t, \t\t\t\t\t2016\">君の名は。\n",
       " \t\t\t\t\n",
       " \t\t\t\t\t, your name.\n",
       " \t\t\t\t\t\n",
       " \t\t\t\t\t, \n",
       " \t\t\t\t\t2016</strong>,\n",
       " <strong class=\"h_movie2\" title=\"君の名は。, your name., 2016\">君の名は。, your name., 2016</strong>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_title = data.select('div[class=mv_info] strong[class=h_movie2]')\n",
    "origin_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'君の名は。\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t, your name.\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t, \\r\\n\\t\\t\\t\\t\\t2016'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_title[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'君の名は。, your name., 2016'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_title[0].text.replace(\"\\t\", '').replace('\\r', '').replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'너의 이름은.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_title = data.select('div[class=mv_info] h3[class=h_movie] a')\n",
    "kor_title[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try - exception practice"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(5):    \n",
    "    try:\n",
    "        s = str(i) if i != 3 else 'a%d' % i    \n",
    "        j = int(s)\n",
    "\n",
    "        print('s = %s, j = %d' % (s, j))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "def get_soup(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers).text\n",
    "        return BeautifulSoup(r, 'lxml')\n",
    "    except Exception as e:\n",
    "        exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "        traceback_details = {\n",
    "                     'filename': exc_traceback.tb_frame.f_code.co_filename,\n",
    "                     'lineno'  : exc_traceback.tb_lineno,\n",
    "                     'name'    : exc_traceback.tb_frame.f_code.co_name,\n",
    "                     'type'    : exc_type.__name__,\n",
    "                     'message' : str(e)\n",
    "                    }\n",
    "        pprint(traceback_details)\n",
    "        return ''\n",
    "\n",
    "# Basic pages\n",
    "def get_basic_page(movie_id):\n",
    "    url = url_basic_base % movie_id\n",
    "    return get_soup(url)\n",
    "\n",
    "def parse_basic_page(page):\n",
    "    movie = {}\n",
    "\n",
    "    score = _parse_main_score(page)\n",
    "    movie['expert_score'] = score[0]\n",
    "    movie['netizen_score'] = score[1]\n",
    "\n",
    "    movie['title'] = _parse_title(page)\n",
    "    movie['e_title'] = _parse_e_title(page)\n",
    "\n",
    "    try:\n",
    "        basic_inf = page.select('dl[class=info_spec]')[0]\n",
    "        movie['genres'] = _parse_genres(page)\n",
    "        movie['countries'] = _parse_countries(page)\n",
    "        movie['running_time'] = _parse_running_time(page)\n",
    "        movie['open_dates'] = _parse_open_date(page)\n",
    "        movie['grade'] = _parse_grade(page)\n",
    "        return movie\n",
    "    except Exception as e:\n",
    "        exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "        traceback_details = {\n",
    "                     'filename': exc_traceback.tb_frame.f_code.co_filename,\n",
    "                     'lineno'  : exc_traceback.tb_lineno,\n",
    "                     'name'    : exc_traceback.tb_frame.f_code.co_name,\n",
    "                     'type'    : exc_type.__name__,\n",
    "                     'message' : str(e)\n",
    "                    }\n",
    "        return movie\n",
    "    \n",
    "def _parse_title(page):\n",
    "    try: \n",
    "        return page.select('div[class=mv_info] h3[class=h_movie] a')[0].text.strip()\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def _parse_e_title(page):\n",
    "    try:\n",
    "        return page.select('div[class=mv_info] strong[class=h_movie2]')[0].text.replace('\\r', '').replace('\\t', '').replace('\\n', '').strip()\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def _parse_genres(page):\n",
    "    genres = page.select('a[href^=/movie/sdb/browsing/bmovie.nhn?genre=]')\n",
    "    return list({genre.text for genre in genres})\n",
    "    \n",
    "def _parse_countries(page):\n",
    "    countries = page.select('a[href^=/movie/sdb/browsing/bmovie.nhn?nation=]')\n",
    "    return list({country.text for country in countries})\n",
    "    \n",
    "def _parse_running_time(page):\n",
    "    running_time = 0\n",
    "    try:\n",
    "        running_time = re.search(r\"\\d+분\", page.text).group()[:-1]\n",
    "    except:\n",
    "        running_time = 0\n",
    "    return running_time\n",
    "\n",
    "def _parse_open_date(page):\n",
    "    return list({d for d in re.findall(r\"\\d+\\.\\d+\\.\\d+ 재*개봉\", page.text)})\n",
    "\n",
    "def _parse_grade(page):\n",
    "    try:\n",
    "        return page.select('a[href^=/movie/sdb/browsing/bmovie.nhn?grade]')[0].text\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def _parse_main_score(page):\n",
    "    try:\n",
    "        main_score = page.select('div[class=main_score]')[0]\n",
    "        expert_score = main_score.select('div[class=spc_score_area] div[class=star_score]')[0].text.replace('\\n','')\n",
    "        netizen_score = main_score.select('div[class=score] div[class=star_score] span[class=st_off]')[0].text.replace('관람객 평점 ', '').replace('점', '')\n",
    "        return expert_score, netizen_score\n",
    "    except Exception as e:\n",
    "        # print('error from _parse_main_score', e)\n",
    "        return -1, -1\n",
    "\n",
    "# def _parse_poster(page):\n",
    "#     try:\n",
    "#         return page.select('img[class=_Img]')[0]\n",
    "#     except:\n",
    "#         return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scraping TIPS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. time.sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134960: 그놈이다\n",
      "134961: 어 라 말라\n",
      "134962: 아이\n",
      "134963: 라라랜드\n",
      "134964: 콜 포 헬프\n"
     ]
    }
   ],
   "source": [
    "for movie_id in range(134960, 134965):\n",
    "    url = url_basic_base % movie_id\n",
    "    page = get_soup(url)\n",
    "    title = _parse_title(page)\n",
    "    print('%d: %s' % (movie_id, title))\n",
    "    time.sleep(1.0) # 1초 기다리기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. User agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "http://www.useragentstring.com/pages/useragentstring.php?name=Chrome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. file download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이썬에서도 음악/사진 파일들을 다운로드 받을 수 있습니다. 다운로드라는 것도 서버와 내 컴퓨터 간의 통로를 열어두고, 전송이 되는 byte 정보들을 모아서 다시 음악, 사진 포멧으로 읽는 것입니다. 물론 스트리밍 서비스들은 temporal하게 내 컴퓨터에 데이터가 쌓이지 않게 막을 수도 있습니다. 그런 종류가 아니라, \\<a>라는 태그로 링크가 걸려있는 이미지 파일들을 다운로드 해보겠습니다. \n",
    "\n",
    "아래 코드는 urllib.request.urlopen을 통하여 서버와 내 컴퓨터 간의 통신 통로를 열어둡니다. while loop 안에서 열려진 통로에서 100000000 byte만큼의 정보를 가져와 buffer에 넣습니다. 그리고 이 정보를 미리 열어둔 downloaded_file이라는 파일에 적습니다. 주고 받은 정보가 텍스트가 아니라 바이트이기 때문에 'wb'로 파일을 열어둡니다. 다운로드가 모두 끝나면 열어둔 통신 서버를 닫고\n",
    "    \n",
    "    opened.close()\n",
    "    \n",
    "열어둔 파일도 닫습니다. \n",
    "\n",
    "    downloaded_file.close()\n",
    "    \n",
    "다운로드가 성공적으로 되었다면 True가, 실패했다면 False가 return 되도록 try - except로 이 코드 부분을 감싸줍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "def download_file(url, fname):\n",
    "    try:\n",
    "        downloaded_file = open(fname, \"wb\")\n",
    "        opened = urllib.request.urlopen(url)\n",
    "        while True:\n",
    "            buffer = opened.read(100000000)\n",
    "            if len(buffer) == 0:\n",
    "                break\n",
    "            downloaded_file.write(buffer)\n",
    "        opened.close()\n",
    "        downloaded_file.close()\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img alt=\"POSTER\" class=\"_Img\" src=\"http://movie.phinf.naver.net/20150123_72/14220061096862xbEP_JPEG/movie_image.jpg?type=m427_320_2\"/>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_img_url = page.select('img[class=_Img]')\n",
    "movie_img_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_file('http://movie.phinf.naver.net/20161125_80/1480051484421z6wDD_JPEG/movie_image.jpg', 'movie_image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_data_file_json.ipynb                \u001b[1m\u001b[36mdata\u001b[m\u001b[m/\r\n",
      "2_pos_tag_with_konlpy.ipynb           google_logo.png\r\n",
      "3_scraping_naver_movie.ipynb          movie_image.jpg\r\n",
      "5_Keyword_extraction.ipynb            \u001b[1m\u001b[36mmypy\u001b[m\u001b[m/\r\n",
      "6_corpus_class.ipynb                  \u001b[1m\u001b[36msoy\u001b[m\u001b[m/\r\n",
      "README.md                             word-movers-distance-in-python.ipynb\r\n",
      "WMD.pdf                               \u001b[1m\u001b[36m강의자료\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
