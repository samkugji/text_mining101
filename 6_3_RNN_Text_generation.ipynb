{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from pprint import pprint\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "corpus = \"\"\n",
    "with open('./data/shakespeare.txt', 'r') as f:\n",
    "    corpus += f.read()\n",
    "corpus = corpus.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('first citizen:\\n'\n",
      " 'before we proceed any further, hear me speak.\\n'\n",
      " '\\n'\n",
      " 'all:\\n'\n",
      " 'speak, speak.\\n'\n",
      " '\\n'\n",
      " 'first citizen:\\n'\n",
      " 'you')\n"
     ]
    }
   ],
   "source": [
    "pprint(corpus[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', ':', 'j', 's', '-', 'p', 'f', 'b', 'w', 'e', 'z', '&', '$', 'o', 'h', 'g', ';', 'k', \"'\", '\\n', 'r', 'q', ' ', 'c', 'm', 'x', '3', '?', 'd', 'a', '!', ',', 'l', 'n', '.', 'y', 'u', 't', 'v']\n"
     ]
    }
   ],
   "source": [
    "# Construct character vocabulary\n",
    "vocab = list(set(corpus))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### help function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_encoding(corpus, vocab):\n",
    "    output = np.zeros((len(corpus), len(vocab)))\n",
    "    \n",
    "    cnt = 0\n",
    "    for char in corpus:\n",
    "        v = [0.0] * len(vocab)\n",
    "        v[vocab.index(char)] = 1.0\n",
    "        output[cnt, :] = v\n",
    "        cnt += 1\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vocab_encoding(corpus=corpus, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model structure parameters\n",
    "in_size = len(vocab) # Size of input vectors at each time step\n",
    "hidden_size = 64 # Size of hidden state vector\n",
    "num_layers = 1 # Number of hidden layers\n",
    "out_size = len(vocab) # Size of output vectors at each time step\n",
    "\n",
    "learning_rate = 0.001 # Learning rate\n",
    "\n",
    "# Data and train parameters\n",
    "batch_size = 64 # Training batch size\n",
    "time_steps = 50 # (Maximum) number of time steps in each batch\n",
    "num_epochs = 1000\n",
    "display_interval = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Make the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholder for inputs: shape [batch_size, timesteps, in_size]\n",
    "X = tf.placeholder(tf.float32, shape=[None, None, in_size], name='input_X')\n",
    "\n",
    "# Placeholder for outputs: shape [batch_size, timesteps, in_size]\n",
    "Y = tf.placeholder(tf.float32, shape=[None, None, out_size], name='target_Y')\n",
    "\n",
    "# Placeholder for initial state\n",
    "state_size = num_layers * 2 * hidden_size\n",
    "hidden_init = tf.placeholder(tf.float32, shape=[None, state_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x11ebeba20>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "hidden_cells = [rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=False) for i in range(num_layers)]\n",
    "hidden = rnn.MultiRNNCell(hidden_cells, state_is_tuple=False)\n",
    "\n",
    "outputs, hidden_new_state = tf.nn.dynamic_rnn(cell=hidden, \n",
    "                                              inputs=X, \n",
    "                                              initial_state=hidden_init, \n",
    "                                              dtype=tf.float32)\n",
    "\n",
    "W = tf.get_variable(name='weights', \n",
    "                    shape=[hidden_size, out_size], \n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.get_variable(name='biases', \n",
    "                    shape=[out_size], \n",
    "                    initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "outputs_reshaped = tf.reshape(outputs, [-1, hidden_size])\n",
    "logits = tf.nn.xw_plus_b(x=outputs_reshaped, weights=W, biases=b, name='logits')\n",
    "\n",
    "batch_time_shape = tf.shape(outputs)\n",
    "outputs_activated = tf.reshape(tensor=tf.nn.softmax(logits), \n",
    "                               shape=[batch_time_shape[0], batch_time_shape[1], out_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_X:0\", shape=(?, ?, 39), dtype=float32)\n",
      "Tensor(\"target_Y:0\", shape=(?, ?, 39), dtype=float32)\n",
      "Tensor(\"rnn/transpose:0\", shape=(?, ?, 64), dtype=float32)\n",
      "Tensor(\"Reshape:0\", shape=(?, 64), dtype=float32)\n",
      "Tensor(\"logits:0\", shape=(?, 39), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(Y)\n",
    "\n",
    "print(outputs)\n",
    "print(outputs_reshaped)\n",
    "\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Cost and training operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_batch_flatten = tf.reshape(Y, [-1, out_size])\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_batch_flatten))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare the session\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize all variables\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Make batches and train them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_x = np.zeros([batch_size, time_steps, in_size])\n",
    "batch_y = np.zeros([batch_size, time_steps, in_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 \tcost:  3.66602\n",
      "epoch:  20 \tcost:  3.27775\n",
      "epoch:  40 \tcost:  3.09292\n",
      "epoch:  60 \tcost:  3.09824\n",
      "epoch:  80 \tcost:  3.06989\n",
      "epoch:  100 \tcost:  3.04243\n",
      "epoch:  120 \tcost:  2.99027\n",
      "epoch:  140 \tcost:  2.96434\n",
      "epoch:  160 \tcost:  2.90161\n",
      "epoch:  180 \tcost:  2.81914\n",
      "epoch:  200 \tcost:  2.77331\n",
      "epoch:  220 \tcost:  2.72813\n",
      "epoch:  240 \tcost:  2.70114\n",
      "epoch:  260 \tcost:  2.64281\n",
      "epoch:  280 \tcost:  2.59593\n",
      "epoch:  300 \tcost:  2.59752\n",
      "epoch:  320 \tcost:  2.56418\n",
      "epoch:  340 \tcost:  2.52903\n",
      "epoch:  360 \tcost:  2.53121\n",
      "epoch:  380 \tcost:  2.5005\n",
      "epoch:  400 \tcost:  2.487\n",
      "epoch:  420 \tcost:  2.4978\n",
      "epoch:  440 \tcost:  2.44718\n",
      "epoch:  460 \tcost:  2.43617\n",
      "epoch:  480 \tcost:  2.406\n",
      "epoch:  500 \tcost:  2.38764\n",
      "epoch:  520 \tcost:  2.38542\n",
      "epoch:  540 \tcost:  2.39884\n",
      "epoch:  560 \tcost:  2.38478\n",
      "epoch:  580 \tcost:  2.39228\n",
      "epoch:  600 \tcost:  2.38125\n",
      "epoch:  620 \tcost:  2.40974\n",
      "epoch:  640 \tcost:  2.36228\n",
      "epoch:  660 \tcost:  2.35699\n",
      "epoch:  680 \tcost:  2.3188\n",
      "epoch:  700 \tcost:  2.32531\n",
      "epoch:  720 \tcost:  2.34441\n",
      "epoch:  740 \tcost:  2.30973\n",
      "epoch:  760 \tcost:  2.2995\n",
      "epoch:  780 \tcost:  2.29118\n",
      "epoch:  800 \tcost:  2.25994\n",
      "epoch:  820 \tcost:  2.31758\n",
      "epoch:  840 \tcost:  2.30369\n",
      "epoch:  860 \tcost:  2.27481\n",
      "epoch:  880 \tcost:  2.2793\n",
      "epoch:  900 \tcost:  2.23476\n",
      "epoch:  920 \tcost:  2.26272\n",
      "epoch:  940 \tcost:  2.26457\n",
      "epoch:  960 \tcost:  2.30499\n",
      "epoch:  980 \tcost:  2.2463\n"
     ]
    }
   ],
   "source": [
    "possible_batch_idx = range(data.shape[0] - time_steps - 1)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    batch_id = random.sample(population=possible_batch_idx, \n",
    "                             k=batch_size)\n",
    "    \n",
    "    for j in range(time_steps):\n",
    "        idx_X = [k + j for k in batch_id]\n",
    "        idx_Y = [k + j + 1 for k in batch_id]\n",
    "        \n",
    "        batch_x[:, j, :] = data[idx_X, :]\n",
    "        batch_y[:, j, :] = data[idx_Y, :]\n",
    "    \n",
    "    init_value = np.zeros((batch_x.shape[0], state_size))\n",
    "    training_cost, _ = sess.run([cost, train_op], feed_dict={X:batch_x, Y:batch_y, hidden_init:init_value})\n",
    "    \n",
    "    if i % display_interval == 0:\n",
    "        print(\"epoch: \", i, \"\\tcost: \", training_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'save/model_rnn_text_generation.ckpt'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, \"save/model_rnn_text_generation.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-allocate 'hidden_last_state'\n",
    "hidden_last_state = np.zeros(state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_op(x, init_zero_state=True):\n",
    "    ## Reset the initial state of the network\n",
    "    if init_zero_state:\n",
    "        init_value = np.zeros(state_size)\n",
    "    else:\n",
    "        init_value = hidden_last_state\n",
    "        \n",
    "    out, hidden_next_state = sess.run([outputs_activated, hidden_new_state], feed_dict={X:[x], hidden_init:[init_value]})\n",
    "    \n",
    "    return out[0][0], hidden_next_state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PREFIX = 'before we proceed any '\n",
    "TEST_PREFIX = TEST_PREFIX.lower()\n",
    "\n",
    "for i in range(len(TEST_PREFIX)):\n",
    "    test_data = vocab_encoding(corpus=TEST_PREFIX[i], vocab=vocab)\n",
    "    out, hidden_last_state = test_op(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE: \n",
      "('before we proceed any blllamonorope raar '\n",
      " 'vofalanyumelaveandu,roreinenofonshearamrofononcloniaeshfofrinlomik '\n",
      " 'afooadyherioanowhoutethather s haluilonherofo '\n",
      " 'wougorufoucerinvepladiiaufoonkeoufaupyogaryouvacellthifopomyachonanofausefalu '\n",
      " 'gizrsha higheroullautheiilunhenofafodtotl inolyularyoreratouqonsenoemarou '\n",
      " 'worafesiilalofrelofoutorpano?wenofohana amanele homyohsavanyothonafri '\n",
      " 'tisthokaanomyecenakiseyowaseesio totene, ltany ishiftae '\n",
      " 'wankisororeatheroullithoro ouroupoundufefugthenaporyonogoponhiofa!\\n'\n",
      " ': oranofovese kthemusim')\n"
     ]
    }
   ],
   "source": [
    "print(\"SENTENCE: \")\n",
    "gen_str = TEST_PREFIX\n",
    "for i in range(500):\n",
    "    element = np.random.choice(range(len(vocab)), p=out)\n",
    "    gen_str += vocab[element]\n",
    "    \n",
    "    out, _ = test_op(vocab_encoding(vocab[element], vocab), init_zero_state=False)\n",
    "pprint(gen_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
