{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load training set and test set\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "newsgroups_test  = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "X_train = newsgroups_train.data\n",
    "Y_train = newsgroups_train.target\n",
    "X_test  = newsgroups_test.data\n",
    "Y_test  = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TRry the SKywatch project in  Arizona.', 'The Vatican library recently made a tour of the US.\\n Can anyone help me in finding a FTP site where this collection is \\n available.', 'Hi there,\\n\\nI am here looking for some help.\\n\\nMy friend is a interior decor designer. He is from Thailand. He is\\ntrying to find some graphics software on PC. Any suggestion on which\\nsoftware to buy,where to buy and how much it costs ? He likes the most\\nsophisticated \\nsoftware(the more features it has,the better)']\n",
      "[2 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_test[0:3])\n",
    "print(Y_test[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare two vectorizers\n",
    "count_vectorizer = CountVectorizer(min_df=40)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting vectorizers to the training set\n",
    "count_vectorizer = count_vectorizer.fit(X_train)\n",
    "tfidf_vectorizer = tfidf_vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform X_train and X_test using 2 vectorizers\n",
    "X_train_count = count_vectorizer.transform(X_train)\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "X_test_count  = count_vectorizer.transform(X_test)\n",
    "X_test_tfidf  = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 758)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 758)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 18)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 29)\t1\n",
      "  (0, 35)\t1\n",
      "  (0, 50)\t1\n",
      "  (0, 55)\t2\n",
      "  (0, 61)\t2\n",
      "  (0, 78)\t1\n",
      "  (0, 87)\t1\n",
      "  (0, 113)\t1\n",
      "  (0, 195)\t2\n",
      "  (0, 244)\t6\n",
      "  (0, 253)\t1\n",
      "  (0, 256)\t2\n",
      "  (0, 260)\t1\n",
      "  (0, 272)\t1\n",
      "  (0, 292)\t1\n",
      "  (0, 301)\t1\n",
      "  (0, 316)\t2\n",
      "  (0, 321)\t3\n",
      "  (0, 329)\t2\n",
      "  (0, 337)\t3\n",
      "  (0, 342)\t1\n",
      "  (0, 354)\t1\n",
      "  (0, 374)\t1\n",
      "  (0, 420)\t1\n",
      "  (0, 453)\t1\n",
      "  (0, 455)\t1\n",
      "  (0, 472)\t1\n",
      "  (0, 503)\t1\n",
      "  (0, 533)\t1\n",
      "  (0, 556)\t1\n",
      "  (0, 559)\t1\n",
      "  (0, 647)\t2\n",
      "  (0, 648)\t7\n",
      "  (0, 649)\t1\n",
      "  (0, 657)\t1\n",
      "  (0, 662)\t1\n",
      "  (0, 671)\t4\n",
      "  (0, 703)\t1\n",
      "  (0, 720)\t1\n",
      "  (0, 729)\t1\n",
      "  (0, 733)\t1\n",
      "  (0, 755)\t3\n",
      "  (0, 756)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_count[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 756)\t0.0721155370883\n",
      "  (0, 755)\t0.14831743135\n",
      "  (0, 733)\t0.0552321132408\n",
      "  (0, 729)\t0.0869038419929\n",
      "  (0, 720)\t0.0779953088406\n",
      "  (0, 703)\t0.0899136950299\n",
      "  (0, 671)\t0.144021224217\n",
      "  (0, 662)\t0.0499187101099\n",
      "  (0, 657)\t0.0631328198798\n",
      "  (0, 649)\t0.0804180650654\n",
      "  (0, 648)\t0.23180983825\n",
      "  (0, 647)\t0.0836810249613\n",
      "  (0, 559)\t0.093868888692\n",
      "  (0, 556)\t0.136566528013\n",
      "  (0, 533)\t0.096000080666\n",
      "  (0, 503)\t0.12460572882\n",
      "  (0, 472)\t0.0753113616121\n",
      "  (0, 455)\t0.106203923252\n",
      "  (0, 453)\t0.0524819891906\n",
      "  (0, 420)\t0.13397450603\n",
      "  (0, 374)\t0.0705501151537\n",
      "  (0, 354)\t0.0720485235024\n",
      "  (0, 342)\t0.0423196121842\n",
      "  (0, 337)\t0.122195764007\n",
      "  (0, 329)\t0.210611295123\n",
      "  (0, 321)\t0.122719069678\n",
      "  (0, 316)\t0.109162363642\n",
      "  (0, 301)\t0.117608096826\n",
      "  (0, 292)\t0.0509033565847\n",
      "  (0, 272)\t0.105976622957\n",
      "  (0, 260)\t0.0599235329153\n",
      "  (0, 256)\t0.252864456363\n",
      "  (0, 253)\t0.0458215515453\n",
      "  (0, 244)\t0.675533177371\n",
      "  (0, 195)\t0.153717311712\n",
      "  (0, 113)\t0.0527484422433\n",
      "  (0, 87)\t0.0492024228549\n",
      "  (0, 78)\t0.106433094352\n",
      "  (0, 61)\t0.104963978381\n",
      "  (0, 55)\t0.172902163822\n",
      "  (0, 50)\t0.0387804606199\n",
      "  (0, 35)\t0.0640215378146\n",
      "  (0, 29)\t0.0931456238322\n",
      "  (0, 19)\t0.0631814559297\n",
      "  (0, 18)\t0.112020595589\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fitting classifiers with count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-define options\n",
    "num_folds = 5\n",
    "num_instances = len(X_train)\n",
    "seed = 1234\n",
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Logistic Regression\n",
    "다음과 같은 파라미터를 컨트롤하여 모델링해봅시다.\n",
    "- regulatization: L1, L2\n",
    "- C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "penalty_set = ['l1', 'l2']\n",
    "C_set = [1, 10]\n",
    "param_grid = dict(penalty=penalty_set, C=C_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:  5.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=4,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': [1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using count vectorizer\n",
    "\n",
    "clf = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=num_folds, n_jobs=-1, verbose=1)\n",
    "# clf = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=num_folds, n_jobs=4, verbose=1)\n",
    "clf.fit(X_train_count, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([   2.3607635 ,    1.71480956,  227.77055197,    1.27476921]),\n",
       " 'mean_score_time': array([ 0.00256724,  0.0089066 ,  0.00244021,  0.00088606]),\n",
       " 'mean_test_score': array([ 0.70648968,  0.70698132,  0.6833825 ,  0.68731563]),\n",
       " 'mean_train_score': array([ 0.92723698,  0.95341819,  0.97209881,  0.97308252]),\n",
       " 'param_C': masked_array(data = [1 1 10 10],\n",
       "              mask = [False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_penalty': masked_array(data = ['l1' 'l2' 'l1' 'l2'],\n",
       "              mask = [False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'C': 1, 'penalty': 'l1'},\n",
       "  {'C': 1, 'penalty': 'l2'},\n",
       "  {'C': 10, 'penalty': 'l1'},\n",
       "  {'C': 10, 'penalty': 'l2'}),\n",
       " 'rank_test_score': array([2, 1, 4, 3], dtype=int32),\n",
       " 'split0_test_score': array([ 0.72303922,  0.73284314,  0.69117647,  0.70343137]),\n",
       " 'split0_train_score': array([ 0.92558426,  0.95448954,  0.9704797 ,  0.97170972]),\n",
       " 'split1_test_score': array([ 0.70098039,  0.70098039,  0.69852941,  0.68627451]),\n",
       " 'split1_train_score': array([ 0.9298893 ,  0.95448954,  0.97293973,  0.97416974]),\n",
       " 'split2_test_score': array([ 0.71007371,  0.69287469,  0.65110565,  0.67321867]),\n",
       " 'split2_train_score': array([ 0.92624462,  0.95759066,  0.97172711,  0.97357099]),\n",
       " 'split3_test_score': array([ 0.68965517,  0.70197044,  0.67980296,  0.67980296]),\n",
       " 'split3_train_score': array([ 0.92628993,  0.9490172 ,  0.97113022,  0.97235872]),\n",
       " 'split4_test_score': array([ 0.70864198,  0.70617284,  0.6962963 ,  0.69382716]),\n",
       " 'split4_train_score': array([ 0.9281768 ,  0.95150399,  0.97421731,  0.97360344]),\n",
       " 'std_fit_time': array([  0.67328645,   0.30278903,  99.07119706,   0.24021393]),\n",
       " 'std_score_time': array([  2.85151694e-03,   6.74953154e-03,   3.21531760e-03,\n",
       "          5.65749957e-05]),\n",
       " 'std_test_score': array([ 0.01100403,  0.01365085,  0.01739054,  0.01057293]),\n",
       " 'std_train_score': array([ 0.00158311,  0.00292363,  0.00133364,  0.00090559])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'C': 1, 'penalty': 'l2'}\n",
      "Best test accuracy :  0.706981317601\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: \", clf.best_params_)\n",
    "print(\"Best test\", scoring, ': ', clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_logistic_count = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. MLPClssifier\n",
    "은닉층의 사이즈를 조절\n",
    "- 은닉층 1개 (노드 수 = 100)\n",
    "- 은닉층 2개 (노드 수 = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier(learning_rate_init=0.01, max_iter=300)\n",
    "\n",
    "hidden_layer_sizes_set = [(100,), (100, 100)]\n",
    "param_grid = dict(hidden_layer_sizes=hidden_layer_sizes_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   11.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.01, max_iter=300, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'hidden_layer_sizes': [(100,), (100, 100)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using count vectorizer\n",
    "clf = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=num_folds, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_train_count, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 1.60793633,  2.81322122]),\n",
       " 'mean_score_time': array([ 0.00458269,  0.01007352]),\n",
       " 'mean_test_score': array([ 0.71042281,  0.71927237]),\n",
       " 'mean_train_score': array([ 0.97271533,  0.97480514]),\n",
       " 'param_hidden_layer_sizes': masked_array(data = [(100,) (100, 100)],\n",
       "              mask = [False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'hidden_layer_sizes': (100,)},\n",
       "  {'hidden_layer_sizes': (100, 100)}),\n",
       " 'rank_test_score': array([2, 1], dtype=int32),\n",
       " 'split0_test_score': array([ 0.70588235,  0.73529412]),\n",
       " 'split0_train_score': array([ 0.97539975,  0.97539975]),\n",
       " 'split1_test_score': array([ 0.72794118,  0.71323529]),\n",
       " 'split1_train_score': array([ 0.97724477,  0.97724477]),\n",
       " 'split2_test_score': array([ 0.71498771,  0.71253071]),\n",
       " 'split2_train_score': array([ 0.97111248,  0.97787339]),\n",
       " 'split3_test_score': array([ 0.69211823,  0.72906404]),\n",
       " 'split3_train_score': array([ 0.96621622,  0.97420147]),\n",
       " 'split4_test_score': array([ 0.71111111,  0.70617284]),\n",
       " 'split4_train_score': array([ 0.97360344,  0.96930632]),\n",
       " 'std_fit_time': array([ 0.61251073,  0.79394513]),\n",
       " 'std_score_time': array([ 0.00019656,  0.00272171]),\n",
       " 'std_test_score': array([ 0.01169781,  0.01100914]),\n",
       " 'std_train_score': array([ 0.00382932,  0.00304371])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'hidden_layer_sizes': (100, 100)}\n",
      "Best test accuracy :  0.719272369715\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: \", clf.best_params_)\n",
    "print(\"Best test\", scoring, ': ', clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_mlp_count = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. 두 모델의 비교\n",
    "Logistic regression에서 가장 성능이 좋은 모델과 MLP에서 가장 성능이 좋은 모델을 선택하여 테스트 데이터에 대한 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_models_count = []\n",
    "best_models_count.append(('LogisticRegression', best_logistic_count))\n",
    "best_models_count.append(('MLPClassifier', best_mlp_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "scores  = []\n",
    "names   = []\n",
    "for name, model in best_models_count:\n",
    "    Y_test_hat = model.predict(X_test_count)\n",
    "    results.append(metrics.confusion_matrix(Y_test, Y_test_hat))\n",
    "    scores.append(metrics.accuracy_score(Y_test, Y_test_hat))\n",
    "    names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LogisticRegression]\n",
      "- test accuracy: 0.660754\n",
      "- confusion matrix :\n",
      " [[170  17  42  90]\n",
      " [ 19 310  47  13]\n",
      " [ 40  38 298  18]\n",
      " [ 88  19  28 116]]\n",
      "\n",
      "[MLPClassifier]\n",
      "- test accuracy: 0.642276\n",
      "- confusion matrix :\n",
      " [[212   9  28  70]\n",
      " [ 21 295  52  21]\n",
      " [ 62  28 270  34]\n",
      " [134  13  12  92]]\n"
     ]
    }
   ],
   "source": [
    "for name, score, cm in list(zip(names, scores, results)):\n",
    "    print('\\n[%s]' % name)\n",
    "    print('- test accuracy: %f' % score)\n",
    "    print('- confusion matrix :\\n', cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fitting classifiers with tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-define options\n",
    "num_folds = 5\n",
    "num_instances = len(X_train)\n",
    "seed = 1234\n",
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Logistic Regression\n",
    "다음과 같은 파라미터를 컨트롤하여 모델링해봅시다.\n",
    "- regulatization: L1, L2\n",
    "- C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "penalty_set = ['l1', 'l2']\n",
    "C_set = [1, 10]\n",
    "param_grid = dict(penalty=penalty_set, C=C_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': [1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using count vectorizer\n",
    "\n",
    "clf = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=num_folds, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_train_count, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([  0.55976553,   0.41338439,  71.22979593,   0.62612696]),\n",
       " 'mean_score_time': array([ 0.0009469 ,  0.00082145,  0.000807  ,  0.00080428]),\n",
       " 'mean_test_score': array([ 0.70698132,  0.70698132,  0.68682399,  0.68731563]),\n",
       " 'mean_train_score': array([ 0.92797431,  0.95341819,  0.97209881,  0.97308252]),\n",
       " 'param_C': masked_array(data = [1 1 10 10],\n",
       "              mask = [False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_penalty': masked_array(data = ['l1' 'l2' 'l1' 'l2'],\n",
       "              mask = [False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'C': 1, 'penalty': 'l1'},\n",
       "  {'C': 1, 'penalty': 'l2'},\n",
       "  {'C': 10, 'penalty': 'l1'},\n",
       "  {'C': 10, 'penalty': 'l2'}),\n",
       " 'rank_test_score': array([1, 1, 4, 3], dtype=int32),\n",
       " 'split0_test_score': array([ 0.7254902 ,  0.73284314,  0.69852941,  0.70343137]),\n",
       " 'split0_train_score': array([ 0.92681427,  0.95448954,  0.9704797 ,  0.97170972]),\n",
       " 'split1_test_score': array([ 0.70833333,  0.70098039,  0.69607843,  0.68627451]),\n",
       " 'split1_train_score': array([ 0.92927429,  0.95448954,  0.97293973,  0.97416974]),\n",
       " 'split2_test_score': array([ 0.6977887 ,  0.69287469,  0.66584767,  0.67321867]),\n",
       " 'split2_train_score': array([ 0.92685925,  0.95759066,  0.97172711,  0.97357099]),\n",
       " 'split3_test_score': array([ 0.69458128,  0.70197044,  0.67980296,  0.67980296]),\n",
       " 'split3_train_score': array([ 0.92874693,  0.9490172 ,  0.97113022,  0.97235872]),\n",
       " 'split4_test_score': array([ 0.70864198,  0.70617284,  0.69382716,  0.69382716]),\n",
       " 'split4_train_score': array([ 0.9281768 ,  0.95150399,  0.97421731,  0.97360344]),\n",
       " 'std_fit_time': array([  0.16603952,   0.06819292,  20.67763845,   0.0860514 ]),\n",
       " 'std_score_time': array([  1.48353964e-04,   2.82554082e-05,   1.14723037e-04,\n",
       "          4.70357258e-05]),\n",
       " 'std_test_score': array([ 0.01082749,  0.01365085,  0.01234153,  0.01057293]),\n",
       " 'std_train_score': array([ 0.00099166,  0.00292363,  0.00133364,  0.00090559])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'C': 1, 'penalty': 'l1'}\n",
      "Best test accuracy :  0.706981317601\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: \", clf.best_params_)\n",
    "print(\"Best test\", scoring, ': ', clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_logistic_count = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. MLPClssifier\n",
    "은닉층의 사이즈를 조절\n",
    "- 은닉층 1개 (노드 수 = 100)\n",
    "- 은닉층 2개 (노드 수 = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier(learning_rate_init=0.01, max_iter=300)\n",
    "\n",
    "hidden_layer_sizes_set = [(100,), (100, 100)]\n",
    "param_grid = dict(hidden_layer_sizes=hidden_layer_sizes_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:   12.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.01, max_iter=300, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=4,\n",
       "       param_grid={'hidden_layer_sizes': [(100,), (100, 100)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using count vectorizer\n",
    "clf = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=num_folds, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_train_tfidf, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 4.26198258,  4.68381529]),\n",
       " 'mean_score_time': array([ 0.00815945,  0.01067963]),\n",
       " 'mean_test_score': array([ 0.72222222,  0.72271386]),\n",
       " 'mean_train_score': array([ 0.96976285,  0.97578726]),\n",
       " 'param_hidden_layer_sizes': masked_array(data = [(100,) (100, 100)],\n",
       "              mask = [False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'hidden_layer_sizes': (100,)},\n",
       "  {'hidden_layer_sizes': (100, 100)}),\n",
       " 'rank_test_score': array([2, 1], dtype=int32),\n",
       " 'split0_test_score': array([ 0.72058824,  0.74754902]),\n",
       " 'split0_train_score': array([ 0.97293973,  0.97539975]),\n",
       " 'split1_test_score': array([ 0.74264706,  0.71078431]),\n",
       " 'split1_train_score': array([ 0.97293973,  0.97785978]),\n",
       " 'split2_test_score': array([ 0.6977887 ,  0.72235872]),\n",
       " 'split2_train_score': array([ 0.95328826,  0.97787339]),\n",
       " 'split3_test_score': array([ 0.71674877,  0.71674877]),\n",
       " 'split3_train_score': array([ 0.97420147,  0.97174447]),\n",
       " 'split4_test_score': array([ 0.73333333,  0.71604938]),\n",
       " 'split4_train_score': array([ 0.97544506,  0.97605893]),\n",
       " 'std_fit_time': array([ 2.19407612,  0.9326872 ]),\n",
       " 'std_score_time': array([ 0.0043067,  0.0052786]),\n",
       " 'std_test_score': array([ 0.01530479,  0.0129707 ]),\n",
       " 'std_train_score': array([ 0.00828959,  0.00224568])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'hidden_layer_sizes': (100, 100)}\n",
      "Best test accuracy :  0.722713864307\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: \", clf.best_params_)\n",
    "print(\"Best test\", scoring, ': ', clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_mlp_count = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. 두 모델의 비교\n",
    "Logistic regression에서 가장 성능이 좋은 모델과 MLP에서 가장 성능이 좋은 모델을 선택하여 테스트 데이터에 대한 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_models_count = []\n",
    "best_models_count.append(('LogisticRegression', best_logistic_count))\n",
    "best_models_count.append(('MLPClassifier', best_mlp_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "scores  = []\n",
    "names   = []\n",
    "for name, model in best_models_count:\n",
    "    Y_test_hat = model.predict(X_test_count)\n",
    "    results.append(metrics.confusion_matrix(Y_test, Y_test_hat))\n",
    "    scores.append(metrics.accuracy_score(Y_test, Y_test_hat))\n",
    "    names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LogisticRegression]\n",
      "- test accuracy: 0.669623\n",
      "- confusion matrix :\n",
      " [[166  16  46  91]\n",
      " [ 18 317  45   9]\n",
      " [ 37  29 313  15]\n",
      " [ 97  21  23 110]]\n",
      "\n",
      "[MLPClassifier]\n",
      "- test accuracy: 0.658537\n",
      "- confusion matrix :\n",
      " [[181  25  29  84]\n",
      " [ 14 315  48  12]\n",
      " [ 31  53 286  24]\n",
      " [102  28  12 109]]\n"
     ]
    }
   ],
   "source": [
    "for name, score, cm in list(zip(names, scores, results)):\n",
    "    print('\\n[%s]' % name)\n",
    "    print('- test accuracy: %f' % score)\n",
    "    print('- confusion matrix :\\n', cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
